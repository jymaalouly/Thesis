# Parameters for AdamOptimizer:
# ==============================================================================
AdamOptimizer.beta1 = 0.9
AdamOptimizer.beta2 = 0.999
AdamOptimizer.epsilon = 1e-08
AdamOptimizer.learning_rate = 0.0001
AdamOptimizer.name = 'Adam'
AdamOptimizer.use_locking = False

# Parameters for annealed_vae:
# ==============================================================================
annealed_vae.c_max = 20
annealed_vae.gamma = 1000
annealed_vae.iteration_threshold = 100000

# Parameters for bernoulli_loss:
# ==============================================================================
bernoulli_loss.subtract_true_image_entropy = False

# Parameters for conv_encoder:
# ==============================================================================
# None.

# Parameters for dataset:
# ==============================================================================
dataset.name = 'scatt'

# Parameters for decoder:
# ==============================================================================
decoder.decoder_fn = @deconv_decoder

# Parameters for deconv_decoder:
# ==============================================================================
# None.

# Parameters for encoder:
# ==============================================================================
encoder.encoder_fn = @conv_encoder
encoder.num_latent = 3

# Parameters for export_as_tf_hub:
# ==============================================================================
export_as_tf_hub.drop_collections = None

# Parameters for model:
# ==============================================================================
model.batch_size = 16
model.eval_steps = 1000
model.model = @annealed_vae()
model.model_num = None
model.name = ''
model.random_seed = 0
model.training_steps = 30000

# Parameters for reconstruction_loss:
# ==============================================================================
reconstruction_loss.activation = 'logits'
reconstruction_loss.loss_fn = @bernoulli_loss

# Parameters for vae_optimizer:
# ==============================================================================
vae_optimizer.learning_rate = None
vae_optimizer.optimizer_fn = @AdamOptimizer
